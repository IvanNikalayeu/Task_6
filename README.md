# Task_6
 Задача: построить ELT pipeline на базе Snowflake с использованием Airflow (local). Общий дизайн архитектуры должен выглядеть таким образом:


Техническое описание: 
Парсим файлик, избавляясь от индексов (иначе Snowflake не прочитает его)
Создаем два data streams в Snowflake и настраиваем их на две таблицы (RAW_TABLE, STAGE_TABLE).
Записываем данные из CSV в RAW_TABLE
Записываем данные из RAW_STREAM в STAGE_TABLE
Записываем данные из STAGE_STREAM в MASTER_TABLE

Дополнительных трансформаций не требуется, но если захочется, можно и добавить (например, на этапе 4). Должно быть разбиение на логические таски DAGа. Docker не нужен, connection к Snowflake можно делать как в Airflow, так и в самом коде. Использовать можно любые операторы и хуки в Airflow.

